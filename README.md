# An efficient open-source design and implementation framework for implementation of non-quantized CNNs on FPGAs

We present an efficient open-source design and implementation framework for deploying non-quantized Convolutional Neural Networks (CNNs) on Field-Programmable Gate Arrays (FPGAs) (available on GitHub). Compared to existing frameworks, that often rely on quantization to improve performance and power efficiency with a significant decrease in accuracy, our approach preserves full precision in all neural network parameters, ensuring high accuracy without compromising computational efficiency. Our solution is built upon the widely used Darknet framework, enabling the integration of CNNs into heterogeneous systems combining CPUs and FPGAs, offering high performance, energy efficiency, and design flexibility. The framework is evaluated on various CNN architectures and a real-world UAV application, demonstrating superior performance and power efficiency compared to CPU and GPU implementations. By addressing the challenges of real-time processing in embedded and edge computing environments, our work paves the way for more robust and efficient AI solutions in power-constrained settings.

## Specifically, the main benefits of our approach can be summarized as follows:

 - Accuracy Preservation: By avoiding quantization and retaining full precision, the proposed framework aims to preserve the accuracy of the CNN models which is crucial for applications where precision is paramount. 
 - High Design Productivity, Flexibility and Adaptability: The presented efficient design flow is based on the widely used DarkNet NN design framework, it  uses purely C/C++ and targets the whole range of FPGAs from the smallest to the largest ones. Since it allows for seamless implementation of virtually any CNN architecture, the proposed framework is fully versatile and adaptable to different application requirements.
 - High Performance: Real-time applications require rapid processing of data to meet stringent latency requirements. The proposed framework can fully exploit the full parallelism of any FPGA to accelerate the inference process of CNNs, ensuring timely and efficient processing. 
 - Energy Efficiency: In many deployment scenarios, particularly in embedded systems and edge devices, power consumption is a critical constraint. The proposed framework optimizes the power efficiency of CNN inference on FPGAs, making it suitable for power-sensitive applications. 
